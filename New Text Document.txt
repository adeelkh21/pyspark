from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Lab02").getOrCreate()
sc = spark.sparkContext

# Parallelize a Python list
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)
print("RDD elements:", rdd.collect())

-------------------------------------------------------------- 

# this can be done to specify the number of partitions you want to create
rdd = sc.parallelize([1,2,3,4,5], 3)   # force 3 partitions

---------------------------------------------------------------

# Map: square each number
squared_rdd = rdd.map(lambda x: x**2)
print("Squared:", squared_rdd.collect())

# Filter: keep even numbers
even_rdd = rdd.filter(lambda x: x % 2 == 0)
print("Even:", even_rdd.collect())

# FlatMap: split sentences into words
sentences = sc.parallelize(["Hello World", "Hello Spark"])
words_rdd = sentences.flatMap(lambda line: line.split(" "))
print("Words:", words_rdd.collect())
-----------------------------------------------------------------
print("Count:", rdd.count())
print("First 3:", rdd.take(3))
print("Sum:", rdd.reduce(lambda a, b: a + b))

--------------------------------------------------------------

text_data = ["Hello World", "Hello Spark", "Hello PySpark"]
rdd = sc.parallelize(text_data)

# Transformations
words = rdd.flatMap(lambda line: line.split(" "))
word_pairs = words.map(lambda word: (word, 1))
word_counts = word_pairs.reduceByKey(lambda a, b: a + b)

# Convert to DataFrame for display
from pyspark.sql import Row
word_counts_df = word_counts.map(lambda x: Row(word=x[0], count=x[1])).toDF()
word_counts_df.show()

---------------------------------------------------------------------

data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)
print(rdd.collect())

# data from texfile
rdd = sc.textFile("hdfs://namenode:9000/data/bigfile.txt")
print(rdd.take(5))   # show first 5 lines

rdd = sc.textFile("s3://my-bucket/data/bigfile.json")

#from database
df = spark.read.format("jdbc").options(
    url="jdbc:mysql://localhost:3306/mydb",
    driver="com.mysql.jdbc.Driver",
    dbtable="big_table",
    user="root",
    password="password").load()

rdd = df.rdd

--------------------------------------------------------
#Partitioning
rdd = sc.textFile("bigdata.txt")
rdd = sc.textFile("bigdata.txt", minPartitions=20)
---------------------------------------------------------
# import findspark
# findspark.init()
from pyspark import SparkContext
from pyspark.sql import SparkSession
sc = SparkContext("local")
spark = SparkSession.builder.getOrCreate()

# parllelize
l = [1,2,3,4,5]
rdd = sc.parallelize(l)
rdd1 = rdd.map(lambda x: x*x)
rdd1.collect()
result = rdd1.reduce(lambda x,y: x+y)
result

# collect
y = rdd.collect()
print(rdd)  # distributed object
print(y)  # not distributed, local data

# take (first n numbers)
x = sc.parallelize([1,3,1,2,3])
y = x.take(num = 3)
print("x=",x.collect())
print(y)

# takeOrdered(in ascending)
x = sc.parallelize([1,3,1,2,3])
y = x.takeOrdered(num = 3)
print("x=",x.collect())
print(y)

# first (first element in rdd)
x = sc.parallelize([1,3,1,2,3])
y = x.first()
print("x=",x.collect())
print('The first element is',y)
x = sc.parallelize([])
# this will return an error
y = x.first()
print("x=",x.collect())
print(y)

# top
x = sc.parallelize([1,3,1,2,4])
y = x.top(num = 3)
print("x=",x.collect())
print("Biggest elements",y)

# top with key function that will return the top 3 smallest elements
y = x.top(num = 3, key = lambda x: -x)
print("Smallest elements",y)

# If `num` is larger than the total number of elements in the RDD, it will return all the elements in descending order.
y = x.top(num = 10)
print("All elements",y)


# -- collectAsMap--

x = sc.parallelize([('C',3),('A',1),('B',2), ('D', 4), ('E', 5)])
y = x.collectAsMap()
print("x=", x.collect())
print("The dictionary of {key:vlaue} pairs:",y)

# If there are duplicate keys, the value of the last key will be retained.
x = sc.parallelize([('C',3),('A',1),('B',2), ('A', 4), ('B', 5)])
y = x.collectAsMap()
print("x=",x.collect())
print("Result if there are duplicates:",y)


--#Map
x = sc.parallelize(["b", "a", "c"])
y = x.map(lambda x: (x, 1))
print("x=",x.collect())  # collect copies RDD elements to a list on the driver
print(y.collect())



# map
x = sc.parallelize([1,2,3]) # sc = spark context, parallelize creates an RDD from the passed object
y = x.map(lambda x: (x,x**2))
print("x=",x.collect())  # collect copies RDD elements to a list on the driver
print(y.collect())



# map
x = sc.parallelize([1,2,3]) # sc = spark context, parallelize creates an RDD from the passed object
y = x.map(lambda x: x**2)
print("x=",x.collect())  # collect copies RDD elements to a list on the driver
print(y.collect())


# Map
x = sc.parallelize([(1,2,3),(2,3,4),(3,4,5)])
y = x.map(lambda x: x)
print("x=",x.collect())
print(y.collect())


# flatMap
x = sc.parallelize([(1,2,3),(2,3,4),(3,4,5)])
y = x.flatMap(lambda x: x)
print("x=",x.collect())
print(y.collect())


# flatMap
x = sc.parallelize([1,2,3])
y = x.flatMap(lambda x: (x, 100*x, x**2))
print("x=",x.collect())
print(y.collect())


# Map
x = sc.parallelize([1,2,3])
y = x.map(lambda x: (x, 100*x, x**2))
print("x=",x.collect())
print(y.collect())


x = sc.parallelize([2, 3, 4])
y = x.flatMap(lambda x: range(1, x))
print("x=",x.collect())
print(y.collect())



# Split sentence into words
lines = sc.parallelize([
    "Apache Spark is a unified analytics engine for large-scale data processing.",
    "It provides high-level APIs in Java, Scala, Python and R",
    "It also supports a rich set of higher-level tools including Spark SQL",
    "MLlib for machine learning",
    "GraphX for graph processing",
    "Structured Streaming for incremental computation and stream processing"
 ])
words = lines.flatMap(lambda x: x.split(' '))
print(lines.collect())
print(words.collect())


# mapValues
x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])
y = x.mapValues(lambda x: [i**2 for i in x]) # function is applied to entire value
print("x=",x.collect())
print(y.collect())

# mapValues
x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])
y = x.map(lambda x: (x[0], [i**2 for i in x[1]])) # function is applied to entire value
print("x=",x.collect())
print(y.collect())


# filter
x = sc.parallelize([1,2,3])
y = x.filter(lambda x: x%2 == 1)  # filters even odd elements
print("x=",x.collect())
print(y.collect())

# distinct
x = sc.parallelize(['A','A','B'])
y = x.distinct()
print("x=",x.collect())
print(y.collect())

# keys
x = sc.parallelize([('C',3),('A',1),('B',2)])
y = x.keys()
print("x=",x.collect())
print(y.collect())

# values
x = sc.parallelize([('C',3),('A',1),('B',2)])
y = x.values()
print("x=",x.collect())
print(y.collect())

# getNumPartitions
x = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14],5)
y = x.getNumPartitions()
print(x.glom().collect())
print(y)


# repartition
x = sc.parallelize([1,2,3,4,5],2)
y = x.repartition(numPartitions=3)
print(x.glom().collect())
print(y.glom().collect())

# coalesce
x = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14], 4)
y = x.coalesce(numPartitions=2)
print(x.glom().collect())
print(y.glom().collect())


# glom
x = sc.parallelize(['C','B','A'], 2)
y = x.glom()
print("x=",x.collect())
print(y.collect())


# reduce (aggregate)
x = sc.parallelize([1,2,3])
y = x.reduce(lambda x, y: x + y)  # computes a cumulative sum
print("x=",x.collect())
print(y)


# fold
x = sc.parallelize([1,2,3])
neutral_zero_value = 0  # 0 for sum, 1 for multiplication
y = x.fold(neutral_zero_value,lambda obj, accumulated: accumulated + obj) # computes cumulative sum
print("x=",x.collect())
print(y)


# aggregate
x = sc.parallelize([2,3,4])
neutral_zero_value = (0,1) # sum: x+0 = x, product: 1*x = x
seqOp = (lambda aggregated, el: (aggregated[0] + el, aggregated[1] * el))
combOp = (lambda aggregated1, aggregated2: (aggregated1[0] + aggregated2[0], aggregated1[1] * aggregated2[1]))
y = x.aggregate(neutral_zero_value,seqOp,combOp)  # computes (cumulative sum, cumulative product)
print("x=",x.collect())
print(y)

# reduceByKey
x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])
y = x.reduceByKey(lambda agg, obj: agg + obj)
print("x=",x.collect())
print(y.collect())
output:
x= [('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]
[('B', 3), ('A', 12)]


# foldByKey
x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])
zeroValue = 1 # one is 'zero value' for multiplication
y = x.foldByKey(zeroValue,lambda agg,x: agg*x )  # computes cumulative product within each key
print("x=",x.collect())
print(y.collect())
output:
x= [('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]
[('B', 2), ('A', 60)]


# aggregateByKey (takes squares of values)
x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])
zeroValue = [] # empty list is 'zero value' for append operation
mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)])
mergeComb = (lambda agg1,agg2: agg1 + agg2 )
y = x.aggregateByKey(zeroValue,mergeVal,mergeComb)
print("x=",x.collect())
print(y.collect())


# groupByKey
x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])
y = x.groupByKey()
print("x=",x.collect())
print([(j[0],[i for i in j[1]]) for j in y.collect()])


# countByKey
x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])
y = x.countByKey()
print("x=",x.collect())
print(y)

# countByValue
x = sc.parallelize([1,3,1,2,3])
y = x.countByValue()
print("x=",x.collect())
print(y)


# join RDDS
x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])
y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])
z = x.join(y)
print("x=",x.collect())
print("y=",y.collect())
print(z.collect())


# leftOuterJoin
x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])
y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])
z = x.leftOuterJoin(y)
print("x=",x.collect())
print(y.collect())
print(z.collect())


# rightOuterJoin
x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])
y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])
z = x.rightOuterJoin(y)
print("x=",x.collect())
print(y.collect())
print(z.collect())


# cartesian product
x = sc.parallelize(['A','B'])
y = sc.parallelize(['C','D'])
z = x.cartesian(y)
print("x=",x.collect())
print(y.collect())
print(z.collect())


# takeSample
x = sc.parallelize(range(7))
ylist = [x.takeSample(withReplacement=False, num=3) for i in range(5)]  # call 'sample' 5 times
print('x = ' + str(x.collect()))
for cnt,y in zip(range(len(ylist)), ylist):
    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y


# count
x = sc.parallelize([1,3,2])
y = x.count()
print("x=",x.collect())
print(y)


# max
x = sc.parallelize([1,3,2,11])
y = x.max()
z = x.max(key=str)
print("x=",x.collect())
print(y)
print(z)


# min
x = sc.parallelize([1,3,2])
y = x.min()
print("x=",x.collect())
print(y)


# sum
x = sc.parallelize([1,3,2])
y = x.sum()
print("x=",x.collect())
print(y)


# mean
x = sc.parallelize([1,3,2])
y = x.mean()
print("x=",x.collect())
print(y)


# variance
x = sc.parallelize([1,3,2])
y = x.variance()  # divides by N
print("x=",x.collect())
print(y)

# stdev
x = sc.parallelize([1,3,2])
y = x.stdev()  # divides by N
print("x=",x.collect())
print(y)


# union
x = sc.parallelize(['A','A','B'])
y = sc.parallelize(['D','C','A'])
z = x.union(y)
print("x=",x.collect())
print("y=",y.collect())
print(z.collect())

# intersection
x = sc.parallelize(['A','A','B'])
y = sc.parallelize(['A','C','D'])
z = x.intersection(y)
print("x=",x.collect())
print(y.collect())
print(z.collect())

# subtract
x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])
y = sc.parallelize([('C',8),('A',2),('D',2)])
z = x.subtract(y)
print("x=",x.collect())
print("x=",y.collect())
print(z.collect())


# zip
x = sc.parallelize(['B','A','A'])
y = x.map(lambda x: ord(x))  # zip expects x and y to have same #partitions and #elements/partition
z = x.zip(y)
print("x=",x.collect())
print(y.collect())
print(z.collect())

# zipWithIndex
x = sc.parallelize(['B','A','A'],2)
y = x.zipWithIndex()
print(x.glom().collect())
print(y.collect())

# sortByKey
x = sc.parallelize([('B',1),('A',2),('C',3)])
y = x.sortByKey()
print("x=",x.collect())
print(y.collect())


# foreach
from __future__ import print_function
x = sc.parallelize([1,2,3])
def f(el):
    '''side effect: append the current RDD elements to a file'''
    f1=open("./foreachExample.txt", 'a+')
    print(el,file=f1)

open('./foreachExample.txt', 'w').close()  # first clear the file contents

y = x.foreach(f) # writes into foreachExample.txt

print("x=",x.collect())
print(y) # foreach returns 'None'
# print the contents of foreachExample.txt
with open("./foreachExample.txt", "r") as foreachExample:
    print (foreachExample.read())



lab 06
# Sample data as given
numbers_list = [3, 1, 4, 1, 5]
key_value_pairs1 = [('A', 1), ('B', 2), ('A', 3), ('A', 4), ('B', 5)]
key_value_pairs2 = [('A', 2), ('A', 2), ('B', 1), ('C', 7)]

# Parallelize into RDDs
numeric_rdd = sc.parallelize(numbers_list)
kv_rdd1 = sc.parallelize(key_value_pairs1)
kv_rdd2 = sc.parallelize(key_value_pairs2)

# Optional: Collect and print for checking
print("Numeric RDD content:", numeric_rdd.collect())
print("KV RDD1 content:", kv_rdd1.collect())
print("KV RDD2 content:", kv_rdd2.collect())

Task 1: Calculate sum via reduce operation
sum_result = numeric_rdd.reduce(lambda a, b: a + b)
print("Sum from reduce operation:", sum_result)

Task 2: Fold-based sum starting from 0
sum_fold_result = numeric_rdd.fold(0, lambda total, num: total + num)
print("Sum via fold (starting 0):", sum_fold_result)

Task 3: Fold to multiply all elements from 1
prod_result = numeric_rdd.fold(1, lambda prod, num: prod * num)
print("Product via fold (starting 1):", prod_result)

Task 4: Use aggregate for sum and count to find average
init_val = (0, 0)
add_func = lambda accum, val: (accum[0] + val, accum[1] + 1)
merge_func = lambda acc_a, acc_b: (acc_a[0] + acc_b[0], acc_a[1] + acc_b[1])

agg_out = numeric_rdd.aggregate(init_val, add_func, merge_func)
calculated_avg = agg_out[0] / agg_out[1]
print("Aggregate output (sum, count):", agg_out)
print("Computed average value:", calculated_avg)

Task 5: Reduce by key to sum values
key_sums = kv_rdd1.reduceByKey(lambda val1, val2: val1 + val2)
print("Per-key sums:", key_sums.collect())

Task 6: Fold by key for product starting from 1
key_products = kv_rdd2.foldByKey(1, lambda prod, val: prod * val)
print("Per-key products:", key_products.collect())

Task 7: Aggregate by key for sum and count
init_key_val = (0, 0)
seq_func = lambda accum, val: (accum[0] + val, accum[1] + 1)
comb_func = lambda acc_a, acc_b: (acc_a[0] + acc_b[0], acc_a[1] + acc_b[1])

key_stats = kv_rdd1.aggregateByKey(init_key_val, seq_func, comb_func)
print("Per-key (sum, count):", key_stats.collect())

Task 8: Aggregate by key for min and max
init_minmax = (float('inf'), float('-inf'))
update_minmax = lambda accum, val: (min(accum[0], val), max(accum[1], val))
merge_minmax = lambda acc_a, acc_b: (min(acc_a[0], acc_b[0]), max(acc_a[1], acc_b[1]))

key_minmax = kv_rdd1.aggregateByKey(init_minmax, update_minmax, merge_minmax)
print("Per-key (min, max):", key_minmax.collect())


lab4 task1
data = [
    ("mardan", 22),("1",20 ),
    ("charsadda", 28),("2",24 ),
    ("chitral", 18),("3", 30),
    ("isb", 30),("4", 27),
    ("peshawar", 35),("5", 28),
    ("karachi", 20),("6",30 ),
    ("lahore", 33),("7", 33),
    ("gilgit ", 25),("8", 19),
    ("multan", 29),("9", 15),
    ("karimabad", 24),("10",21 ),
    ("tirah", 23),("11", 19),
    ("mohmand", 15),("12", 15),
    ("RYK", 40),("13",21 ),
]


# creating RDD from the dataset
rdd_temp_celsius = sc.parallelize(data)

#printing 5 elements of the RDD
print(rdd_temp_celsius.take(5))


#mapValue fxn applied to convert C to F
rdd_temp_fahrenheit = rdd_temp_celsius.mapValues(lambda c: c * 9/5 + 32)

#verifying the results
print(rdd_temp_fahrenheit.take(5))

#verifying if keys are unchnaged after mapValue fxn
print("Distinct cities before transformation:")
print(rdd_temp_celsius.keys().distinct().collect())

print("Distinct cities after transformation:")
print(rdd_temp_fahrenheit.keys().distinct().collect())


task 2

#TASK#2
data2 = list(range(1, 40)) 
#dataset generation
rdd_partitioned = sc.parallelize(data2, 3)

#printing the inital partition count
print("Initial partition count:", rdd_partitioned.getNumPartitions())
y = rdd_partitioned.glom()
print(y.collect())

#function that will be applied to each partition
def partition_contents(index, iterator):
    yield (index, list(iterator))  
# Repartition the RDD into 6 partitions
rdd_repartitioned = rdd_partitioned.repartition(6)

# Print the contents of each partition along with its index
print("Contents per partition:")
print(rdd_repartitioned.mapPartitionsWithIndex(partition_contents).collect())


#Coalescing from 6 down to 2 partitions
rdd_coalesced = rdd_repartitioned.coalesce(2)
print("contents per partition after coalesce:")
print(rdd_coalesced.mapPartitionsWithIndex(partition_contents).collect())




#printing the number of elements in each partition at each stage (3,6,2)
def partition_count(index, iterator):
    yield (index, len(list(iterator)))

print("Number of elements per partition (3 partitions):")
print(rdd_partitioned.mapPartitionsWithIndex(partition_count).collect())

print("Number of elements per partition (6 partitions):")
print(rdd_repartitioned.mapPartitionsWithIndex(partition_count).collect())

print("Number of elements per partition (2 partitions):")
print(rdd_coalesced.mapPartitionsWithIndex(partition_count).collect())


lab 5 
task 1


#RDD with some fruit names containing duplicates
fruit_list = ["Apple", "Banana", "Banana","Apple","Orange","Banana","Grapes","Mango","Banana","Banana","pinaple","pinaple","tomato"]
rdd_fruits = sc.parallelize(fruit_list)

# Removing duplicates using the distinct fxn
rdd_unique_fruits = rdd_fruits.distinct()



# Show the final list of unique fruits
unique_fruits = rdd_unique_fruits.collect()
print("Unique Fruits:", unique_fruits)

# Show the count of unique fruits
unique_fruit_count = rdd_unique_fruits.count()
print("Number of Unique Fruits:", unique_fruit_count)

#Task2

# Class A and B student ID lists
list_A = [101, 102, 103, 104, 105,110,112,120,125]
list_B = [103, 106, 107, 108, 109,125,102,101,113]

# Create RDDs for class lists A and B
rdd_A = sc.parallelize(list_A)
rdd_B = sc.parallelize(list_B)

# Combine both RDDs using the union operation
rdd_combined = rdd_A.union(rdd_B)

# Show the final list of combined student IDs
combined_ID = rdd_combined.collect()
print("Combined IDs:", combined_ID)

# Remove duplicates from the combined RDD
rdd_unique_ids = rdd_combined.distinct()



# Show the final list of combined unique student IDs
unique_ID = rdd_unique_ids.collect()
print("Combined IDs:", unique_ID)

# Sort the RDD and display the unique student IDs in ascending order
sorted_ids = rdd_unique_ids.sortBy(lambda x: x).collect()
print("Unique Student IDs in Ascending Order:", sorted_ids)

#Task3

# Employee dataset (emp, dept, amount)
rdd = sc.parallelize([("A", "IT", 200), 
                      ("B", "IT", 300),
                      ("C", "HR", 150),
                      ("D", "HR", 400)])


# Compute total count of records
total_count = rdd.count()
print("Total Rows:", total_count)

# Group by department and count the number of records in each department
from pyspark.sql import Row
from pyspark import SQLContext

sqlContext = SQLContext(sc)
rdd_rows = rdd.map(lambda x: Row(emp=x[0], dept=x[1], amount=x[2]))
df = sqlContext.createDataFrame(rdd_rows)

# Count records per department, sorted by department ascending
dept_count = df.groupBy("dept").count().orderBy("dept").collect()
for row in dept_count:
    print(f"Department: {row['dept']}, Count: {row['count']}")

# Calculate the sum and average of the 'amount' column
total_revenue = df.agg({"amount": "sum"}).collect()[0][0]
average_revenue = df.agg({"amount": "avg"}).collect()[0][0]
print(f"Total Revenue: {total_revenue}")
print(f"Average Revenue: {average_revenue}")

from pyspark.sql import functions as F  # Import functions as F

# Perform aggregation and rename the columns for clarity
min_max_sales = df.groupBy("dept").agg(
    F.min("amount").alias("min_amount"), 
    F.max("amount").alias("max_amount")
).collect()

# Print the results
for row in min_max_sales:
    print(f"Department: {row['dept']}, Min Sale: {row['min_amount']}, Max Sale: {row['max_amount']}")











